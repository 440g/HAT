/home/mk/git/HAT/fairseq/modules/sinusoidal_positional_embedding.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.weights is None or max_pos > self.weights.size(0):
/home/mk/git/HAT/fairseq/models/transformer_super.py:394: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not encoder_padding_mask.any():
/home/mk/git/HAT/fairseq/modules/multihead_attention_super.py:263: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
/home/mk/git/HAT/fairseq/modules/multihead_attention_super.py:293: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
/home/mk/git/HAT/fairseq/modules/multihead_attention_super.py:295: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (self.onnx_trace and attn.size(1) == 1):
| Configs: Namespace(configs='configs/wmt14.en-de/convert_onnx/super.yml', pdb=False, no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='label_smoothed_cross_entropy', optimizer='adam', lr_scheduler='cosine', task='translation', num_workers=10, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, src_dict_path=None, tgt_dict_path=None, model_path='downloaded_models/HAT_wmt16ende_super_space0.pt', beam=5, arch='transformersuper_wmt_en_de', no_token_positional_embeddings=False, get_attn=False, encoder_embed_choice=[640, 512], decoder_embed_choice=[640, 512], encoder_layer_num_choice=[6], decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], encoder_ffn_embed_dim_choice=[3072, 2048, 1024], decoder_ffn_embed_dim_choice=[3072, 2048, 1024], encoder_self_attention_heads_choice=[8, 4], decoder_self_attention_heads_choice=[8, 4], decoder_ende_attention_heads_choice=[8, 4], qkv_dim=512, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], vocab_original_scaling=False, encoder_embed_dim_subtransformer=None, decoder_embed_dim_subtransformer=None, encoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_all_subtransformer=None, encoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_arbitrary_ende_attn_all_subtransformer=None, label_smoothing=0.1, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, warmup_updates=10000, warmup_init_lr=1e-07, max_lr=0.001, t_mult=1, lr_period_updates=-1, lr_shrink=1.0, data='data/binary/wmt16_en_de', source_lang=None, target_lang=None, lazy_load=False, raw_text=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, share_all_embeddings=True, dropout=0.3, attention_dropout=0.1, encoder_embed_dim=640, decoder_embed_dim=640, encoder_ffn_embed_dim=3072, decoder_ffn_embed_dim=3072, encoder_layers=6, decoder_layers=6, encoder_attention_heads=8, decoder_attention_heads=8, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=640, decoder_input_dim=640)
| Buildng model transformersuper_wmt_en_de...
| [en] dictionary: 32768 types
| [de] dictionary: 32768 types
| Fallback to xavier initializer
| Exporting model to ONNX...
Traceback (most recent call last):
  File "/home/mk/git/HAT/convert_onnx.py", line 92, in <module>
    main()
  File "/home/mk/git/HAT/convert_onnx.py", line 82, in main
    export_to_onnx(
  File "/home/mk/git/HAT/convert_onnx.py", line 49, in export_to_onnx
    torch.onnx.export(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/onnx/utils.py", line 551, in export
    _export(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/onnx/utils.py", line 1648, in _export
    graph, params_dict, torch_out = _model_to_graph(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/onnx/utils.py", line 1170, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/onnx/utils.py", line 1046, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/onnx/utils.py", line 950, in _trace_and_get_graph_from_model
    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/jit/_trace.py", line 1497, in _get_trace_graph
    outs = ONNXTracedModule(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/jit/_trace.py", line 141, in forward
    graph, out = torch._C._create_graph_by_tracing(
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/jit/_trace.py", line 132, in wrapper
    outs.append(self.inner(*trace_inputs))
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1543, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/home/mk/git/HAT/fairseq/models/fairseq_model.py", line 222, in forward
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1543, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/home/mk/git/HAT/fairseq/models/transformer_super.py", line 401, in forward
    x = layer(x, encoder_padding_mask)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1543, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/home/mk/git/HAT/fairseq/models/transformer_super.py", line 900, in forward
    x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1543, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/home/mk/git/HAT/fairseq/modules/multihead_attention_super.py", line 301, in forward
    attn = self.out_proj(attn)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mk/miniconda3/envs/rknn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1543, in _slow_forward
    result = self.forward(*input, **kwargs)
  File "/home/mk/git/HAT/fairseq/modules/linear_super.py", line 58, in forward
    return F.linear(x, self.samples['weight'], self.samples['bias'])
RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient
Tensor:
Columns 1 to 6 5.2654e-02 -2.5555e-02  6.3955e-02  2.6899e-02  2.1934e-02 -2.1669e-02
-4.3726e-03  6.7866e-02  2.1100e-02  1.1604e-02 -3.4295e-03 -1.2981e-03
 1.9818e-02 -3.5226e-02 -3.1740e-02 -7.2057e-02 -4.6456e-02  4.1625e-03
-6.0681e-02 -4.5354e-02 -2.9682e-02 -2.3449e-02 -3.0763e-02 -1.3436e-02
생략

Columns 7 to 12-6.7885e-02 -6.1797e-02  1.7018e-02  4.9302e-02 -5.2939e-02  6.3248e-02
-1.2946e-02 -6.5291e-02 -2.6385e-02 -5.3994e-02 -5.2861e-02 -3.9288e-02
-3.3564e-02  3.2547e-02  4.9065e-02 -6.3903e-02 -4.2359e-02  6.9462e-02
 6.2701e-02  5.6702e-02  3.8288e-02  6.7552e-02 -5.4527e-02 -3.5672e-02
 6.4061e-02 -2.4346e-02 -3.9133e-02 -3.2286e-04  1.5823e-02  6.2137e-02
생략

Columns 13 to 18-6.0902e-02 -4.5265e-02 -4.9637e-02 -3.4135e-02 -6.0008e-02  4.5968e-02
-4.1238e-02 -6.2319e-02  1.4529e-02  4.5366e-04  4.0897e-02 -4.9999e-03
-6.0120e-02 -2.0127e-02 -6.3787e-02 -2.1085e-02 -4.0123e-02  1.5304e-02
 5.3828e-02  4.5800e-02 -6.8371e-02  1.6286e-02  5.8270e-02 -2.8063e-02
생략

Columns 19 to 24-2.9598e-02 -8.8776e-03 -7.6874e-04 -7.2088e-02 -4.0058e-02  4.3920e-02
 4.2529e-02 -6.3252e-02 -6.4053e-02 -5.8259e-02  1.9061e-02 -9.2618e-03
-1.8214e-02 -1.6179e-02 -1.6896e-02  1.7970e-02  5.3538e-02  3.0584e-02
 2.3226e-02 -1.4189e-02 -7.3636e-03  6.4690e-02 -3.2685e-02 -4.2817e-02
 5.6030e-02  5.6524e-02  1.1277e-02  3.2686e-02  4.8328e-02 -2.0387e-02
-5.3174e-02 -3.4038e-02 -6.5118e-02  5.2382e-02 -8.9708e-03 -3.7703e-02
-4.8935e-02  4.5438e-02  2.6109e-02 -4.6482e-03 -3.9024e-02 -2.3799e-02
~

